# --- gpt.py ---
from dataclasses import dataclass 
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


@dataclass
class GPTConfig:
    """Configuration for GPT model architecture."""
    context_length: int = 1024  # Maximum sequence length
    vocab_size: int = 50257     # Size of vocabulary
    num_layers: int = 12        # Number of transformer blocks
    embd_size: int = 768        # Embedding dimension
    num_heads: int = 12         # Number of attention heads

class CausalSelfAttention(nn.Module):
    """Multi-head causal self-attention layer for autoregressive modeling."""

    def __init__(self, config):
        super().__init__()
        # Ensure embedding dimension is evenly divisible across all attention heads
        assert config.embd_size % config.num_heads == 0, f"embedding dim should be divisible by number of heads"
        # Key, query, value projections for all heads in a single batch operation
        self.c_attn = nn.Linear(config.embd_size, 3 * config.embd_size)
        # Output projection to bring multi-head results back to embedding dimension
        self.c_proj = nn.Linear(config.embd_size, config.embd_size)
        self.n_head = config.num_heads
        self.n_embed = config.embd_size
    
    def forward(self, x):
        B, T, C = x.size()  # Batch size, sequence length, embedding dimension
        
        # Compute query, key, value for all heads in one go
        qkv = self.c_attn(x)
        q, k, v = qkv.split(self.n_embed, dim=2)
        
        # Reshape and transpose to separate heads: (B, T, C) -> (B, n_head, T, head_size)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)

        # Flash Attention: Fused, memory-efficient attention
        y = F.scaled_dot_product_attention(
            q, k, v,
            attn_mask=None,
            dropout_p=0.0,
            is_causal=True
        )
        
        # Concatenate all heads back together: (B, nh, T, hs) -> (B, T, C)
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        
        # Final output projection
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    """Feed-forward network with GELU activation (4x expansion factor)."""

    def __init__(self, config):
        super().__init__()
        # Expand to 4x embedding dimension
        self.c_fc = nn.Linear(config.embd_size, 4 * config.embd_size)
        self.gelu = nn.GELU()
        # Project back to embedding dimension
        self.c_proj = nn.Linear(4 * config.embd_size, config.embd_size)
    
    def forward(self, x):
        x = self.c_fc(x)      # Expand
        x = self.gelu(x)       # Non-linear activation
        x = self.c_proj(x)     # Project back
        return x

class Block(nn.Module):
    """Transformer block: LayerNorm -> Attention -> LayerNorm -> MLP, with residual connections."""
    
    def __init__(self, config):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.embd_size)  # Pre-normalization for attention
        self.attn = CausalSelfAttention(config)
        self.ln_2 = nn.LayerNorm(config.embd_size)  # Pre-normalization for MLP
        self.mlp = MLP(config)
        self.attn_scale = 1 / (2 * config.num_layers) ** 0.5
    
    def forward(self, x):
        # Attention with residual connection (pre-norm style)
        x = x + self.attn_scale * self.attn(self.ln_1(x))
        # MLP with residual connection (pre-norm style)
        x = x + self.mlp(self.ln_2(x))
        return x

class GPT(nn.Module):
    """GPT Language Model: token + position embeddings -> transformer blocks -> language modeling head."""

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.embd_size),    # Token embeddings
            wpe = nn.Embedding(config.context_length, config.embd_size),  # Position embeddings
            h = nn.ModuleList([Block(config) for _ in range(config.num_layers)]),  # Transformer blocks
            ln_f = nn.LayerNorm(config.embd_size)  # Final layer norm before output
        ))
        # Language modeling head: maps embeddings back to vocabulary logits
        self.lm_head = nn.Linear(config.embd_size, config.vocab_size, bias=False)

        # weight sharing scheme (reduces 768*50267=~40M params, fewer params, more efficient)
        self.transformer.wte.weight = self.lm_head.weight

        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        if isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, idx, targets, return_logits=None):
        # idx is of shape (B, T)
        B, T = idx.size()
        assert T <= self.config.context_length, f"Cannot forward sequence of length {T}, block size is only {self.config.context_length}"
        # forward the token and posisition embeddings
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)
        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)
        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)
        x = tok_emb + pos_emb
        # forward the blocks of the transformer
        for block in self.transformer.h:
            x = block(x)
        # forward the final layernorm and the classifier
        x = self.transformer.ln_f(x)
        logits = self.lm_head(x)  # (B, T, vocab_size)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        if return_logits:
            return logits, loss
        return None, loss

"""
FineWeb Preprocessing Script (10B Sample)

This script downloads, tokenizes, and shards the FineWeb 10B dataset.
It stores the results in .bin files, which includes a 1024-byte header.

Usage Examples:

1. Dev Mode (20M tokens):
   python prepare.py --total_tokens 20000000 --shard_size 2000000
   -> Output will be in 'fineweb20M'

2. Full Training:
   python prepare.py --shard_size 100000000
   -> Output will be in 'fineweb10B'
"""

# --- prepare.py ---
import os
import argparse
import numpy as np
import tiktoken
from datasets import load_dataset
from tqdm import tqdm

MAGIC = 20240520  
VERSION = 1       
HEADER_SIZE = 256 

def write_shard(path, tokens: np.ndarray):
    """Writes a list of tokens to a binary file with a header."""
    assert tokens.dtype == np.uint16
    
    header = np.zeros(HEADER_SIZE, dtype=np.int32)
    header[0] = MAGIC
    header[1] = VERSION
    header[2] = len(tokens) 

    with open(path, "wb") as f:
        f.write(header.tobytes())  
        f.write(tokens.tobytes())  

def main():
    parser = argparse.ArgumentParser(description="Clean and simple FineWeb preprocessing")
    parser.add_argument("--shard_size", type=int, default=100_000_000, 
                        help="How many tokens per shard")
    parser.add_argument("--total_tokens", type=int, default=None,
                        help="How many tokens to process in total (leave empty for full dataset)")
    parser.add_argument("--out_dir", type=str, default=None,
                        help="Output directory (defaults to fineweb20M or fineweb10B)")
    args = parser.parse_args()

    remote_name = "sample-10BT"
    
    if args.out_dir:
        out_dir = args.out_dir
    elif args.total_tokens == 20_000_000:
        out_dir = "data/fineweb20M"
    else:
        out_dir = "data/fineweb10B"
        
    os.makedirs(out_dir, exist_ok=True)

    print(f"--- FineWeb Preprocessing (10B Sample) ---")
    print(f"Output Directory : {out_dir}")
    print(f"Total Tokens     : {args.total_tokens or 'ENTIRE DATASET'}")
    print(f"Tokens per Shard : {args.shard_size:,}")


    dataset = load_dataset(
        "HuggingFaceFW/fineweb",
        name=remote_name,
        split="train",
        streaming=True,
    )

    enc = tiktoken.get_encoding("gpt2")
    eot = enc._special_tokens['<|endoftext|>'] 

    def tokenize(text: str) -> np.ndarray:
        """Tokenizes a string and adds an <|endoftext|> prefix."""
        tokens = [eot]
        tokens.extend(enc.encode_ordinary(text))
        return np.array(tokens, dtype=np.uint16)

    progress = tqdm(
        total=args.total_tokens,
        unit="tokens",
        desc="Processing tokens",
        disable=args.total_tokens is None,
    )

    shard_idx = 0
    shard = np.empty(args.shard_size, dtype=np.uint16)
    shard_pos = 0
    total_processed = 0
    
    total_limit = args.total_tokens if args.total_tokens is not None else float('inf')

    for doc in dataset:
        if total_processed >= total_limit:
            break
            
        tokens = tokenize(doc["text"])
        pos = 0
        
        while pos < len(tokens) and total_processed < total_limit:
            space_left = args.shard_size - shard_pos
            
            if space_left == 0:
                shard_path = os.path.join(out_dir, f"fineweb_{shard_idx:06d}.bin")
                write_shard(shard_path, shard)
                shard_idx += 1
                shard_pos = 0
                space_left = args.shard_size
            
            take = min(space_left, len(tokens) - pos, total_limit - total_processed)
            
            shard[shard_pos:shard_pos + take] = tokens[pos:pos + take]
            
            shard_pos += take
            pos += take
            total_processed += take
            progress.update(take)

    if shard_pos > 0:
        shard_path = os.path.join(out_dir, f"fineweb_{shard_idx:06d}.bin")
        write_shard(shard_path, shard[:shard_pos])

    progress.close()

if __name__ == "__main__":
    main()

# --- dataloader.py ---
import numpy as np
import torch
import os

def load_tokens_bin(filename, dtype=np.uint16, offset_bytes=256):
    npt = np.memmap(
        filename,
        dtype=dtype,
        mode="r",
        offset=offset_bytes
    )
    return torch.from_numpy(npt.astype(np.int32)).long()

class TokenDataLoader:
    
    def __init__(self, data_root, B, T):
        self.B = B
        self.T = T

        shards = [
            os.path.join(data_root, f)
            for f in sorted(os.listdir(data_root))
        ]
        assert len(shards) > 0, f"No shards found"

        self.shards = shards
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.tokens = load_tokens_bin(self.shards[self.current_shard])
        self.current_position = 0

    def _load_next_shard(self):
        self.current_shard = (self.current_shard + 1) % len(self.shards)
        self.tokens = load_tokens_bin(self.shards[self.current_shard])
        self.current_position = 0

    def next_batch(self):
        B, T = self.B, self.T
        needed = B * T + 1

        if self.current_position + needed > len(self.tokens):
            self._load_next_shard()

        buf = self.tokens[self.current_position : self.current_position + needed]

        x = buf[:-1].view(B, T)
        y = buf[1:].view(B, T)

        self.current_position += B * T
        return x, y

# --- train.py ---
import os
import time
import torch
import torch.nn as nn
from model.gpt import GPT, GPTConfig
from data.dataloader import TokenDataLoader

# Hyperparameters
total_batch_size = 524288 # 2**19 tokens per optimizer step
batch_size = 32 # micro-batch size
sequence_length = 1024
max_steps = 38 # 20M tokens / 524288 approx 38 steps
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Using device: {device}")

# Calculate gradient accumulation steps
assert total_batch_size % (batch_size * sequence_length) == 0, "total_batch_size must be divisible by batch_size * sequence_length"
grad_accum_steps = total_batch_size // (batch_size * sequence_length)
print(f"Total batch size: {total_batch_size}")
print(f"Tokens per micro-batch: {batch_size * sequence_length}")
print(f"Gradient accumulation steps: {grad_accum_steps}")

# Data Loader
data_root = "data/fineweb20M"
if not os.path.exists(data_root):
    print(f"Warning: {data_root} not found.")

train_loader = TokenDataLoader(data_root, B=batch_size, T=sequence_length)

torch.set_float32_matmul_precision('high')

# Model
config = GPTConfig(context_length=sequence_length)
model = GPT(config)
model.to(device)
model = torch.compile(model) # Enable torch.compile 

# Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

# Training Loop
model.train()
step = 0

# Start timing total training
total_start_time = time.time()

while step < max_steps:
    t0 = time.time()
    optimizer.zero_grad()
    loss_accum = 0.0
    
    for micro_step in range(grad_accum_steps):
        # Get batch
        x, y = train_loader.next_batch()
        x, y = x.to(device), y.to(device)
        
        # Forward pass with autocast
        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
            _, loss = model(x, y)
            loss = loss / grad_accum_steps
            loss_accum += loss.detach()
        
        # Backward pass
        loss.backward()
    
    optimizer.step()
    
    # Wait for GPU to finish all queued operations for accurate timing
    if device == "cuda": torch.cuda.synchronize()
        
    t1 = time.time()
    dt = (t1 - t0) * 1000 # milliseconds
    tokens_per_sec = total_batch_size / (t1 - t0)
    
    # Log: step:0 train_loss:10.8299 train_time:0ms token/s:
    print(f"step:{step}/{max_steps} train_loss:{loss_accum.item():.4f} train_time:{dt:.0f}ms token/s:{tokens_per_sec:.2f}")
    
    step += 1

# End timing total training
total_end_time = time.time()
total_time = total_end_time - total_start_time
print(f"Total time: {total_time:.2f}s ({total_time/60:.2f} minutes)")
print(f"Total tokens processed: {max_steps * total_batch_size:,}")
print(f"Average tokens/sec: {(max_steps * total_batch_size) / total_time:.2f}")

# --- OUTPUT ---
Using device: cuda
Total batch size: 524288
Tokens per micro-batch: 32768
Gradient accumulation steps: 16
step:0/38 train_loss:10.9811 train_time:63757ms token/s:8223.23
step:1/38 train_loss:10.0204 train_time:2565ms token/s:204392.05
step:2/38 train_loss:9.5983 train_time:2567ms token/s:204206.35
step:3/38 train_loss:9.3707 train_time:2567ms token/s:204212.00
step:4/38 train_loss:9.1274 train_time:2571ms token/s:203950.78
step:5/38 train_loss:8.9345 train_time:2570ms token/s:203992.58
step:6/38 train_loss:8.7165 train_time:2572ms token/s:203870.67
step:7/38 train_loss:8.5576 train_time:2582ms token/s:203055.18
step:8/38 train_loss:8.4108 train_time:2585ms token/s:202827.01
step:9/38 train_loss:8.1779 train_time:2586ms token/s:202776.25
step:10/38 train_loss:8.0887 train_time:2584ms token/s:202860.86
step:11/38 train_loss:7.9242 train_time:2585ms token/s:202799.12
step:12/38 train_loss:7.8992 train_time:2583ms token/s:202976.33
step:13/38 train_loss:7.7645 train_time:2587ms token/s:202691.30
step:14/38 train_loss:7.7466 train_time:2587ms token/s:202661.51
step:15/38 train_loss:7.6071 train_time:2586ms token/s:202715.05
step:16/38 train_loss:7.5515 train_time:2586ms token/s:202741.94
step:17/38 train_loss:7.5500 train_time:2589ms token/s:202536.87
step:18/38 train_loss:7.5043 train_time:2587ms token/s:202653.77
step:19/38 train_loss:7.4510 train_time:2591ms token/s:202369.70
step:20/38 train_loss:7.4028 train_time:2595ms token/s:202033.22
step:21/38 train_loss:7.3799 train_time:2590ms token/s:202453.19
step:22/38 train_loss:7.4127 train_time:2590ms token/s:202452.65
step:23/38 train_loss:7.3183 train_time:2592ms token/s:202284.87
step:24/38 train_loss:7.3717 train_time:2598ms token/s:201828.37
step:25/38 train_loss:7.3021 train_time:2591ms token/s:202372.89
step:26/38 train_loss:7.3730 train_time:2594ms token/s:202081.91
step:27/38 train_loss:7.3101 train_time:2596ms token/s:201939.56
step:28/38 train_loss:7.2800 train_time:2599ms token/s:201706.50
step:29/38 train_loss:7.2480 train_time:2594ms token/s:202123.54
step:30/38 train_loss:7.1669 train_time:2597ms token/s:201915.44
step:31/38 train_loss:7.1701 train_time:2594ms token/s:202108.21
step:32/38 train_loss:7.1682 train_time:2595ms token/s:202010.20
step:33/38 train_loss:7.1825 train_time:2601ms token/s:201585.37
step:34/38 train_loss:7.0589 train_time:2600ms token/s:201660.83
step:35/38 train_loss:7.1818 train_time:2591ms token/s:202376.50
step:36/38 train_loss:7.1133 train_time:2602ms token/s:201529.93
step:37/38 train_loss:7.0395 train_time:2596ms token/s:201937.30
Total time: 159.51s (2.66 minutes)
Total tokens processed: 19,922,944
Average tokens/sec: 124904.15
